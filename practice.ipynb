{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from model import MODEL\n",
    "from run import train, test\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from dataloader import getDataLoader\n",
    "from earlystop import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = 0\n",
    "patience = 5\n",
    "max_iter = 100  # number of iterations\n",
    "init_lr = 0.005  # initial learning rate\n",
    "lr_decay = 0.75  # learning rate decay\n",
    "final_lr = 1E-5  # learning rate will not decrease after hitting this threshold\n",
    "dropout = 0.3\n",
    "exercise_embed_dim = 64  # hidden state dim for LSTM\n",
    "hidden_dim = 64\n",
    "layer_num = 2  # layer number for LSTM\n",
    "max_step = 100  # the allowed maximum length of a sequence\n",
    "maxgradnorm = 50.0  # maximum gradient norm\n",
    "Lamda = 1  # hyper-parameter Lamda\n",
    "alpha = 0.2  # alpha\n",
    "num_heads = 4  # number of head attentions\n",
    "mode = 3  # mode of integration function\n",
    "# 1: ca / 2: mul / 3: ca mul / 4: rasch\n",
    "fold = '1'  # number of fold\n",
    "dataset = 'assist2009_B'\n",
    "\n",
    "#when dataset is assist2009_B\n",
    "batch_size = 32\n",
    "n_knowledge_concept = 110  # the kinds of unique questions in the dataset\n",
    "n_exercise = 16891  # the number of unique questions in the dataset\n",
    "data_dir = './data/assist2009_B'  # data directory\n",
    "data_name = 'assist2009_B'  # data set name\n",
    "\n",
    "memory_size = n_knowledge_concept\n",
    "lr = init_lr\n",
    "memory_key_state_dim = exercise_embed_dim\n",
    "memory_value_state_dim = exercise_embed_dim * 2\n",
    "\n",
    "train_data_path = data_dir + \"/\" + data_name + \"_train\" + fold + \".csv\"\n",
    "valid_data_path = data_dir + \"/\" + data_name + \"_valid\" + fold + \".csv\"\n",
    "test_data_path = data_dir + \"/\" + data_name + \"_test\" + fold + \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "class DataReader():\n",
    "    def __init__(self, train_path, valid_path, test_path, maxstep, numofques):\n",
    "        self.train_path = train_path\n",
    "        self.valid_path = valid_path\n",
    "        self.test_path = test_path\n",
    "        self.maxstep = maxstep\n",
    "        self.numofques = numofques\n",
    "\n",
    "    def getTrainData(self):\n",
    "        print('loading train data...')\n",
    "        q_data= []\n",
    "        qa_data =[]\n",
    "        p_data= []\n",
    "        trainData = []\n",
    "        batch = 0\n",
    "        with open(self.train_path, 'r') as train:\n",
    "            for student, problem, ques, ans in itertools.zip_longest(*[train] * 4):\n",
    "                batch = batch +1\n",
    "                try:\n",
    "                    problem = [int(p) for p in problem.strip().strip(',').split(',')]\n",
    "                    ques = [int(q) for q in ques.strip().strip(',').split(',')]\n",
    "                    ans = [int(a) for a in ans.strip().strip(',').split(',')]\n",
    "                except:\n",
    "                    print(batch)\n",
    "                length = np.size(ques)\n",
    "                slices = length//self.maxstep + (1 if length%self.maxstep > 0 else 0)\n",
    "                for i in range(slices):\n",
    "                    q_temp = np.zeros(shape=[self.maxstep])\n",
    "                    qa_temp = np.zeros(shape=[self.maxstep])\n",
    "                    p_temp = np.zeros(shape=[self.maxstep])\n",
    "                    if length > 0:\n",
    "                        if length >= self.maxstep:\n",
    "                            l = self.maxstep\n",
    "                        else:\n",
    "                            l = length\n",
    "                        for j in range(l):\n",
    "                            q_temp[j] = ques[i*self.maxstep + j]\n",
    "                            qa_temp[j] = ans[i*self.maxstep + j]\n",
    "                            p_temp[j] = problem[i*self.maxstep + j]\n",
    "                        length = length - self.maxstep\n",
    "                    q_data.append(q_temp.tolist())\n",
    "                    qa_data.append(qa_temp.tolist())\n",
    "                    p_data.append(p_temp.tolist())\n",
    "            print('train--question done: ' + str(np.array(q_data).shape))\n",
    "            print('train--question_ans done: ' + str(np.array(qa_data).shape))\n",
    "            print('train--problem done: ' + str(np.array(p_data).shape))\n",
    "            return np.array(q_data).astype(float), np.array(qa_data).astype(float), np.array(p_data).astype(float)\n",
    "\n",
    "    def getValidData(self):\n",
    "        print('loading valid data...')\n",
    "        q_data= []\n",
    "        qa_data =[]\n",
    "        p_data= []\n",
    "        validData = []\n",
    "        batch = 0\n",
    "        with open(self.valid_path, 'r') as valid:\n",
    "            for student, problem, ques, ans in itertools.zip_longest(*[valid] * 4):\n",
    "                batch = batch +1\n",
    "                try:\n",
    "                    problem = [int(p) for p in problem.strip().strip(',').split(',')]\n",
    "                    ques = [int(q) for q in ques.strip().strip(',').split(',')]\n",
    "                    ans = [int(a) for a in ans.strip().strip(',').split(',')]\n",
    "                except:\n",
    "                    print(batch)\n",
    "                length = np.size(ques)\n",
    "                slices = length//self.maxstep + (1 if length%self.maxstep > 0 else 0)\n",
    "                for i in range(slices):\n",
    "                    q_temp = np.zeros(shape=[self.maxstep])\n",
    "                    qa_temp = np.zeros(shape=[self.maxstep])\n",
    "                    p_temp = np.zeros(shape=[self.maxstep])\n",
    "                    if length > 0:\n",
    "                        if length >= self.maxstep:\n",
    "                            l = self.maxstep\n",
    "                        else:\n",
    "                            l = length\n",
    "                        for j in range(l):\n",
    "                            q_temp[j] = ques[i*self.maxstep + j]\n",
    "                            qa_temp[j] = ans[i*self.maxstep + j]\n",
    "                            p_temp[j] = problem[i*self.maxstep + j]\n",
    "                        length = length - self.maxstep\n",
    "                    q_data.append(q_temp.tolist())\n",
    "                    qa_data.append(qa_temp.tolist())\n",
    "                    p_data.append(p_temp.tolist())\n",
    "            print('valid--question done: ' + str(np.array(q_data).shape))\n",
    "            print('valid--question_ans done: ' + str(np.array(qa_data).shape))\n",
    "            print('valid--problem done: ' + str(np.array(p_data).shape))\n",
    "            return np.array(q_data).astype(float), np.array(qa_data).astype(float), np.array(p_data).astype(float)\n",
    "\n",
    "\n",
    "    def getTestData(self):\n",
    "        print('loading test data...')\n",
    "        testData = []\n",
    "        q_data = []\n",
    "        qa_data = []\n",
    "        p_data = []\n",
    "        zero = [0 for i in range(self.numofques * 2)]\n",
    "        batch = 0\n",
    "        with open(self.test_path, 'r') as test:\n",
    "            for student, problem, ques, ans in itertools.zip_longest(*[test] * 4):\n",
    "                #length = int(length.strip().strip(','))\n",
    "                batch = batch + 1\n",
    "                try:\n",
    "                    problem = [int(p) for p in problem.strip().strip(',').split(',')]\n",
    "                    ques = [int(q) for q in ques.strip().strip(',').split(',')]\n",
    "                    ans = [int(a) for a in ans.strip().strip(',').split(',')]\n",
    "                except:\n",
    "                    print(batch)\n",
    "                length = np.size(ques)\n",
    "                slices = length // self.maxstep + (1 if length % self.maxstep > 0 else 0)\n",
    "                for i in range(slices):\n",
    "                    q_temp = np.zeros(shape=[self.maxstep])\n",
    "                    qa_temp = np.zeros(shape=[self.maxstep])\n",
    "                    p_temp = np.zeros(shape=[self.maxstep])\n",
    "                    if length > 0:\n",
    "                        if length >= self.maxstep:\n",
    "                            l = self.maxstep\n",
    "                        else:\n",
    "                            l = length\n",
    "                        for j in range(l):\n",
    "                            q_temp[j] = ques[i * self.maxstep + j]\n",
    "                            qa_temp[j] = ans[i * self.maxstep + j]\n",
    "                            p_temp[j] = problem[i * self.maxstep + j]\n",
    "                        length = length - self.maxstep\n",
    "                    q_data.append(q_temp.tolist())\n",
    "                    qa_data.append(qa_temp.tolist())\n",
    "                    p_data.append(p_temp.tolist())\n",
    "            print('test--question done: ' + str(np.array(q_data).shape))\n",
    "            print('test--question_ans done: ' + str(np.array(qa_data).shape))\n",
    "            print('test--problem done: ' + str(np.array(p_data).shape))\n",
    "            return np.array(q_data).astype(float), np.array(qa_data).astype(float), np.array(p_data).astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/')\n",
    "from readdata import DataReader\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def getDataLoader(train_data_path, valid_data_path, test_data_path, max_step, n_knowledge_concept):\n",
    "    handle = DataReader(train_data_path, valid_data_path, test_data_path, max_step, n_knowledge_concept)\n",
    "\n",
    "    kc_data_train, respose_data_train, exercise_data_train = handle.getTrainData()\n",
    "    kc_data_valid, respose_data_valid, exercise_data_valid = handle.getValidData()\n",
    "    kc_data_test, respose_data_test, exercise_data_test = handle.getTestData()\n",
    "\n",
    "    return kc_data_train, respose_data_train, exercise_data_train,kc_data_valid, respose_data_valid, exercise_data_valid, kc_data_test, respose_data_test, exercise_data_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train data...\n",
      "train--question done: (3293, 100)\n",
      "train--question_ans done: (3293, 100)\n",
      "train--problem done: (3293, 100)\n",
      "loading valid data...\n",
      "valid--question done: (1038, 100)\n",
      "valid--question_ans done: (1038, 100)\n",
      "valid--problem done: (1038, 100)\n",
      "loading test data...\n",
      "test--question done: (1032, 100)\n",
      "test--question_ans done: (1032, 100)\n",
      "test--problem done: (1032, 100)\n"
     ]
    }
   ],
   "source": [
    "train_kc_data, train_respond_data, train_exercise_data, \\\n",
    "        valid_kc_data, valid_respose_data, valid_exercise_data, \\\n",
    "        test_kc_data, test_respose_data, test_exercise_data \\\n",
    "        = getDataLoader(train_data_path, valid_data_path, test_data_path, max_step, n_knowledge_concept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_exercise_respond_data = train_respond_data * \\\n",
    "    n_exercise + train_exercise_data\n",
    "valid_exercise_respose_data = valid_respose_data * \\\n",
    "    n_exercise + valid_exercise_data\n",
    "test_exercise_respose_data = test_respose_data * \\\n",
    "    n_exercise + test_exercise_data\n",
    "adj_exercise_kc = np.loadtxt(data_dir + \"/exercise_kc_map.txt\")\n",
    "adj_exercise_kc = utils.varible(torch.from_numpy(adj_exercise_kc), gpu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3293"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.random.permutation(train_exercise_data.shape[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import utils as utils\n",
    "import embedd_loss as embedd_loss\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_exercise = n_exercise\n",
    "n_kc = n_knowledge_concept\n",
    "batch_size = batch_size\n",
    "exercise_embed_dim = exercise_embed_dim\n",
    "student_num = None\n",
    "nheads = num_heads\n",
    "alpha = alpha\n",
    "dropout = dropout\n",
    "# params = params\n",
    "mode = mode\n",
    "Lamda = Lamda\n",
    "hidden_dim = hidden_dim\n",
    "layer_num = layer_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exercise_KC_GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, mode, concat=True):\n",
    "        super(Exercise_KC_GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "        self.mode =mode\n",
    "        self.W1 = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.kaiming_normal(self.W1)\n",
    "\n",
    "        if mode == 1 or mode == 3:\n",
    "            self.reduceDim = nn.Linear(in_features * 2, self.out_features, bias=True)\n",
    "            nn.init.kaiming_normal(self.reduceDim.weight)\n",
    "            nn.init.constant(self.reduceDim.bias, 0)\n",
    "        if mode != 1:\n",
    "            self.E = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "            nn.init.kaiming_normal(self.E)\n",
    "        if mode ==4:\n",
    "            self.U = nn.Parameter(torch.empty(size=(in_features, 1)))\n",
    "            nn.init.kaiming_normal(self.U)\n",
    "\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
    "        nn.init.kaiming_normal(self.a)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, exercise_h, kc_h, adj_exercise_kc):\n",
    "        if self.concat:\n",
    "            kc_Wh = torch.mm(kc_h, self.W1)\n",
    "            exercise_Wh = torch.mm(exercise_h, self.W1)\n",
    "            a_input = self._prepare_attentional_mechanism_input(kc_Wh, exercise_Wh)\n",
    "            e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "            zero_vec = -9e15 * torch.ones_like(e)\n",
    "            attention = torch.where(adj_exercise_kc > 0, e, zero_vec)\n",
    "            attention = F.softmax(attention, dim=1)\n",
    "            new_kc_embed = torch.matmul(attention, kc_Wh)\n",
    "\n",
    "            if self.mode == 1:\n",
    "                exercises_embedd = torch.cat((new_kc_embed, exercise_h), dim=1)\n",
    "                exercises_embedd = self.reduceDim(exercises_embedd)\n",
    "            if self.mode == 2:\n",
    "                exercise_Eh = torch.mm(exercise_h, self.E)\n",
    "                exercises_embedd = new_kc_embed.mul(exercise_Eh)\n",
    "            if self.mode == 3:\n",
    "                exercise_Eh = torch.mm(exercise_h, self.E)\n",
    "                exercises_embedd = torch.cat((new_kc_embed, new_kc_embed.mul(exercise_Eh)), dim=1)\n",
    "                exercises_embedd = self.reduceDim(exercises_embedd)\n",
    "            if self.mode == 4:\n",
    "                u = torch.mm(exercise_h, self.U)\n",
    "                d_kt = torch.mm(new_kc_embed, self.E)\n",
    "                exercises_embedd = new_kc_embed + u * d_kt\n",
    "            return F.elu(exercises_embedd)\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, kc_Wh, exercise_Wh):\n",
    "        N_kc = kc_Wh.size()[0]\n",
    "        N_exercise = exercise_Wh.size()[0]\n",
    "        Wh_repeated_in_chunks = exercise_Wh.repeat_interleave(N_kc, dim=0)\n",
    "        Wh_repeated_alternating = kc_Wh.repeat(N_exercise, 1)\n",
    "        all_combinations_matrix = torch.cat([Wh_repeated_in_chunks, Wh_repeated_alternating], dim=1)\n",
    "        return all_combinations_matrix.view(N_exercise, N_kc, 2 * self.out_features)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(exercise_embed_dim * 2, hidden_dim,\n",
    "               layer_num, batch_first=True, dropout=dropout)\n",
    "fc1 = nn.Linear(exercise_embed_dim + hidden_dim, hidden_dim, bias=True)\n",
    "fc2 = nn.Linear(hidden_dim, 1, bias=True)\n",
    "rel = nn.ReLU()\n",
    "\n",
    "exercise_embed = nn.Parameter(torch.randn(n_exercise, exercise_embed_dim))\n",
    "kc_embed = nn.Parameter(torch.randn(n_kc, exercise_embed_dim))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_650/995484167.py:8: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(W1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1202,  0.0214, -0.0699,  ...,  0.0381, -0.3796,  0.1299],\n",
       "        [-0.2248, -0.1405,  0.1015,  ..., -0.1020,  0.0535, -0.0760],\n",
       "        [-0.2335,  0.0681, -0.0628,  ..., -0.1022,  0.0834, -0.4411],\n",
       "        ...,\n",
       "        [-0.0351, -0.3010,  0.0809,  ..., -0.2111, -0.2437,  0.0782],\n",
       "        [ 0.3125, -0.3680, -0.0881,  ...,  0.0310, -0.0940,  0.3209],\n",
       "        [-0.2887, -0.1861, -0.1339,  ...,  0.0710, -0.0222,  0.0392]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dropout = dropout\n",
    "in_features = exercise_embed_dim\n",
    "out_features = exercise_embed_dim\n",
    "alpha = alpha\n",
    "concat = True\n",
    "mode = mode\n",
    "W1 = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "nn.init.kaiming_normal(W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_650/1761267380.py:2: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(reduceDim.weight)\n",
      "/tmp/ipykernel_650/1761267380.py:3: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(reduceDim.bias, 0)\n",
      "/tmp/ipykernel_650/1761267380.py:6: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(E)\n",
      "/tmp/ipykernel_650/1761267380.py:9: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(U)\n",
      "/tmp/ipykernel_650/1761267380.py:12: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(a)\n"
     ]
    }
   ],
   "source": [
    "reduceDim = nn.Linear(in_features * 2, out_features, bias=True)\n",
    "nn.init.kaiming_normal(reduceDim.weight)\n",
    "nn.init.constant(reduceDim.bias, 0)\n",
    "\n",
    "E = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "nn.init.kaiming_normal(E)\n",
    "\n",
    "U = nn.Parameter(torch.empty(size=(in_features, 1)))\n",
    "nn.init.kaiming_normal(U)\n",
    "\n",
    "a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
    "nn.init.kaiming_normal(a)\n",
    "\n",
    "leakyrelu = nn.LeakyReLU(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16891, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exercise_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _prepare_attentional_mechanism_input(kc_Wh, exercise_Wh):\n",
    "    N_kc = kc_Wh.size()[0]\n",
    "    N_exercise = exercise_Wh.size()[0]\n",
    "    Wh_repeated_in_chunks = exercise_Wh.repeat_interleave(N_kc, dim=0)\n",
    "    Wh_repeated_alternating = kc_Wh.repeat(N_exercise, 1)\n",
    "    all_combinations_matrix = torch.cat(\n",
    "        [Wh_repeated_in_chunks, Wh_repeated_alternating], dim=1)\n",
    "    return all_combinations_matrix.view(N_exercise, N_kc, 2 * out_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 99])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kc_one_seq = train_respond_data[0:32, :]\n",
    "exercise_one_seq = train_exercise_data[0:32, :]\n",
    "exercise_respond_batch_seq = train_exercise_respond_data[0:32, :]\n",
    "target = train_exercise_respond_data[0:32, :]\n",
    "target = (target - 1) / n_exercise\n",
    "target = np.floor(target)\n",
    "input_kc = utils.varible(torch.LongTensor(kc_one_seq), gpu)\n",
    "input_exercise = utils.varible(torch.LongTensor(exercise_one_seq), gpu)\n",
    "input_exercise_respond = utils.varible(\n",
    "    torch.LongTensor(exercise_respond_batch_seq), gpu)\n",
    "target = utils.varible(torch.FloatTensor(target), gpu)\n",
    "target.shape\n",
    "target = target[:, 1:]\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 2, 1, 0],\n",
      "        [0, 4, 2, 0],\n",
      "        [2, 0, 4, 0]])\n",
      "tensor([[3, 2, 1, 0],\n",
      "        [3, 2, 1, 0],\n",
      "        [3, 2, 1, 0],\n",
      "        [0, 4, 2, 0],\n",
      "        [0, 4, 2, 0],\n",
      "        [0, 4, 2, 0],\n",
      "        [2, 0, 4, 0],\n",
      "        [2, 0, 4, 0],\n",
      "        [2, 0, 4, 0]])\n",
      "tensor([[3, 3, 3, 2, 2, 2, 1, 1, 1, 0, 0, 0],\n",
      "        [0, 0, 0, 4, 4, 4, 2, 2, 2, 0, 0, 0],\n",
      "        [2, 2, 2, 0, 0, 0, 4, 4, 4, 0, 0, 0]])\n",
      "tensor([[3, 2, 1, 0],\n",
      "        [0, 4, 2, 0],\n",
      "        [2, 0, 4, 0],\n",
      "        [3, 2, 1, 0],\n",
      "        [0, 4, 2, 0],\n",
      "        [2, 0, 4, 0],\n",
      "        [3, 2, 1, 0],\n",
      "        [0, 4, 2, 0],\n",
      "        [2, 0, 4, 0]])\n",
      "tensor([[3, 2, 1, 0],\n",
      "        [0, 4, 2, 0],\n",
      "        [2, 0, 4, 0],\n",
      "        [3, 2, 1, 0],\n",
      "        [3, 2, 1, 0],\n",
      "        [3, 2, 1, 0],\n",
      "        [0, 4, 2, 0],\n",
      "        [0, 4, 2, 0],\n",
      "        [0, 4, 2, 0],\n",
      "        [2, 0, 4, 0],\n",
      "        [2, 0, 4, 0],\n",
      "        [2, 0, 4, 0]])\n",
      "tensor([[3, 2, 1, 0, 3, 3, 3, 2, 2, 2, 1, 1, 1, 0, 0, 0],\n",
      "        [0, 4, 2, 0, 0, 0, 0, 4, 4, 4, 2, 2, 2, 0, 0, 0],\n",
      "        [2, 0, 4, 0, 2, 2, 2, 0, 0, 0, 4, 4, 4, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(np.random.randint(5, size = (3,4)))\n",
    "print(a)\n",
    "b = a.repeat_interleave(3, dim=0)\n",
    "print(b)\n",
    "c = a.repeat_interleave(3, dim=1)\n",
    "print(c)\n",
    "d = a.repeat(3,1)\n",
    "print(d)\n",
    "e = torch.cat([a, b], dim=0)\n",
    "print(e)\n",
    "f = torch.cat([a, c], dim=1)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([110, 64])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exercise_embed = nn.Parameter(torch.randn(n_exercise, exercise_embed_dim))\n",
    "kc_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_embed.shape\n",
    "exercise_embed.shape\n",
    "kc_Wh = torch.mm(kc_embed, W1)\n",
    "exercise_Wh = torch.mm(exercise_embed, W1)\n",
    "N_kc = kc_Wh.size()[0]\n",
    "N_exercise = exercise_Wh.size()[0]\n",
    "Wh_repeated_in_chunks = exercise_Wh.repeat_interleave(N_kc, dim=0)\n",
    "# 110개의 knowledge_concept에 대해 문항별 weight 생성\n",
    "Wh_repeated_alternating = kc_Wh.repeat(N_exercise, 1)\n",
    "# 16891개의 exercise에 대해 kc별 weight 생성\n",
    "all_combinations_matrix = torch.cat(\n",
    "    [Wh_repeated_in_chunks, Wh_repeated_alternating], dim=1)\n",
    "a_input = all_combinations_matrix.view(N_exercise, N_kc, 2*out_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1858010, 64])\n",
      "torch.Size([1858010, 64])\n",
      "torch.Size([16891, 110, 128])\n"
     ]
    }
   ],
   "source": [
    "print(Wh_repeated_alternating.shape)\n",
    "print(Wh_repeated_in_chunks.shape)\n",
    "print(a_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_650/2829987827.py:2: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(a)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = nn.Parameter(torch.empty(size=(2*64, 1)))\n",
    "nn.init.kaiming_normal(a)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16891, 110, 128])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([16891, 110])\n",
      "torch.Size([16891, 110])\n",
      "torch.Size([16891, 110])\n",
      "torch.Size([110, 64])\n"
     ]
    }
   ],
   "source": [
    "print(a_input.shape)\n",
    "print(a.shape)\n",
    "torch.matmul(a_input, a).shape\n",
    "e = leakyrelu(torch.matmul(a_input, a).squeeze(2)).cuda()\n",
    "zero_vec = -9e15 * torch.ones_like(e).cuda()\n",
    "print(e.shape)\n",
    "print(zero_vec.shape)\n",
    "adj_exercise_kc.shape\n",
    "attention = torch.where(adj_exercise_kc > 0, e, zero_vec)\n",
    "print(attention.shape)\n",
    "print(kc_Wh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = leakyrelu(torch.matmul(a_input, a).squeeze(2)).cuda()\n",
    "zero_vec = -9e15 * torch.ones_like(e).cuda()\n",
    "attention = torch.where(adj_exercise_kc > 0, e, zero_vec)\n",
    "attention = F.softmax(attention, dim=1)\n",
    "new_kc_embed = torch.matmul(attention, kc_Wh.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16891, 64])\n"
     ]
    }
   ],
   "source": [
    "print(new_kc_embed.shape)\n",
    "# Exercise_KC_GraphAttentionLayer(self.exercise_embed_dim, self.exercise_embed_dim, dropout=self.dropout, alpha=self.alpha, mode=self.mode,\n",
    "# concat=True) for _ in range(self.nheads)]\n",
    "kc_h = kc_embed\n",
    "exercise_h = exercise_embed\n",
    "kc_Wh = torch.mm(kc_h, W1) #(110, 64)\n",
    "exercise_Wh = torch.mm(exercise_h, W1) #(16891, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16891, 128])\n",
      "torch.Size([16891, 64])\n"
     ]
    }
   ],
   "source": [
    "exercise_Eh = torch.mm(exercise_h, E)\n",
    "exercises_embedd = torch.cat(\n",
    "    (new_kc_embed, new_kc_embed.mul(exercise_Eh.cuda())), dim=1)\n",
    "print(exercises_embedd.shape)\n",
    "reduceDim = reduceDim.cuda()\n",
    "exercises_embedd = reduceDim(exercises_embedd)\n",
    "print(exercises_embedd.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exercise_Wh' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m exercise_Wh\u001b[39m.\u001b[39mrepeat_interleave(\u001b[39m110\u001b[39m, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mshape\n\u001b[1;32m      2\u001b[0m \u001b[39m# 110개의 knowledge_concept에 대해 문항별 weight 생성\u001b[39;00m\n\u001b[1;32m      3\u001b[0m kc_Wh\u001b[39m.\u001b[39mrepeat(\u001b[39m16891\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mshape\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exercise_Wh' is not defined"
     ]
    }
   ],
   "source": [
    "exercise_Wh.repeat_interleave(110, dim=0).shape\n",
    "# 110개의 knowledge_concept에 대해 문항별 weight 생성\n",
    "kc_Wh.repeat(16891, 1).shape\n",
    "# 16891개의 exercise에 대해 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise_KC_GraphAttentionLayer(self.exercise_embed_dim, self.exercise_embed_dim, dropout=self.dropout, alpha=self.alpha, mode=self.mode,\n",
    "# concat=True) for _ in range(self.nheads)]\n",
    "kc_h = kc_embed\n",
    "exercise_h = exercise_embed\n",
    "kc_Wh = torch.mm(kc_h, W1) #(110, 64)\n",
    "exercise_Wh = torch.mm(exercise_h, W1) #(16891, 64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Exercise_KC_GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout, alpha, mode, concat=True):\n",
    "        super(Exercise_KC_GraphAttentionLayer, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "        self.mode = mode\n",
    "        self.W1 = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
    "        nn.init.kaiming_normal(self.W1)\n",
    "\n",
    "        if mode == 1 or mode == 3:\n",
    "            self.reduceDim = nn.Linear(\n",
    "                in_features * 2, self.out_features, bias=True)\n",
    "            nn.init.kaiming_normal(self.reduceDim.weight)\n",
    "            nn.init.constant(self.reduceDim.bias, 0)\n",
    "        if mode != 1:\n",
    "            self.E = nn.Parameter(torch.empty(\n",
    "                size=(in_features, out_features)))\n",
    "            nn.init.kaiming_normal(self.E)\n",
    "        if mode == 4:\n",
    "            self.U = nn.Parameter(torch.empty(size=(in_features, 1)))\n",
    "            nn.init.kaiming_normal(self.U)\n",
    "\n",
    "        self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
    "        nn.init.kaiming_normal(self.a)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "\n",
    "    def forward(self, exercise_h, kc_h, adj_exercise_kc):\n",
    "        if self.concat:\n",
    "            kc_Wh = torch.mm(kc_h, self.W1)\n",
    "            exercise_Wh = torch.mm(exercise_h, self.W1)\n",
    "            a_input = self._prepare_attentional_mechanism_input(\n",
    "                kc_Wh, exercise_Wh)\n",
    "            e = self.leakyrelu(torch.matmul(a_input, self.a).squeeze(2))\n",
    "            zero_vec = -9e15 * torch.ones_like(e)\n",
    "            attention = torch.where(adj_exercise_kc > 0, e, zero_vec)\n",
    "            attention = F.softmax(attention, dim=1)\n",
    "            new_kc_embed = torch.matmul(attention, kc_Wh)\n",
    "\n",
    "            if self.mode == 1:\n",
    "                exercises_embedd = torch.cat((new_kc_embed, exercise_h), dim=1)\n",
    "                exercises_embedd = self.reduceDim(exercises_embedd)\n",
    "            if self.mode == 2:\n",
    "                exercise_Eh = torch.mm(exercise_h, self.E)\n",
    "                exercises_embedd = new_kc_embed.mul(exercise_Eh)\n",
    "            if self.mode == 3:\n",
    "                exercise_Eh = torch.mm(exercise_h, self.E)\n",
    "                exercises_embedd = torch.cat(\n",
    "                    (new_kc_embed, new_kc_embed.mul(exercise_Eh)), dim=1)\n",
    "                exercises_embedd = self.reduceDim(exercises_embedd)\n",
    "            if self.mode == 4:\n",
    "                u = torch.mm(exercise_h, self.U)\n",
    "                d_kt = torch.mm(new_kc_embed, self.E)\n",
    "                exercises_embedd = new_kc_embed + u * d_kt\n",
    "            return F.elu(exercises_embedd)\n",
    "\n",
    "    def _prepare_attentional_mechanism_input(self, kc_Wh, exercise_Wh):\n",
    "        N_kc = kc_Wh.size()[0]\n",
    "        N_exercise = exercise_Wh.size()[0]\n",
    "        Wh_repeated_in_chunks = exercise_Wh.repeat_interleave(N_kc, dim=0)\n",
    "        Wh_repeated_alternating = kc_Wh.repeat(N_exercise, 1)\n",
    "        all_combinations_matrix = torch.cat(\n",
    "            [Wh_repeated_in_chunks, Wh_repeated_alternating], dim=1)\n",
    "        return all_combinations_matrix.view(N_exercise, N_kc, 2 * self.out_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_650/348885505.py:11: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(self.W1)\n",
      "/tmp/ipykernel_650/348885505.py:16: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(self.reduceDim.weight)\n",
      "/tmp/ipykernel_650/348885505.py:17: UserWarning: nn.init.constant is now deprecated in favor of nn.init.constant_.\n",
      "  nn.init.constant(self.reduceDim.bias, 0)\n",
      "/tmp/ipykernel_650/348885505.py:21: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(self.E)\n",
      "/tmp/ipykernel_650/348885505.py:27: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(self.a)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected condition, x and y to be on the same device, but condition is on cuda:0 and x and y are on cpu and cpu respectively",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[172], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m exercise_kc_attentions \u001b[39m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     Exercise_KC_GraphAttentionLayer(exercise_embed_dim, exercise_embed_dim, dropout\u001b[39m=\u001b[39mdropout, alpha\u001b[39m=\u001b[39malpha, mode\u001b[39m=\u001b[39mmode, concat\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nheads)]\n\u001b[0;32m----> 3\u001b[0m exercise_embedding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([att(exercise_embed, kc_embed, adj_exercise_kc) \u001b[39mfor\u001b[39;00m att \u001b[39min\u001b[39;00m exercise_kc_attentions], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[172], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m exercise_kc_attentions \u001b[39m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     Exercise_KC_GraphAttentionLayer(exercise_embed_dim, exercise_embed_dim, dropout\u001b[39m=\u001b[39mdropout, alpha\u001b[39m=\u001b[39malpha, mode\u001b[39m=\u001b[39mmode, concat\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(nheads)]\n\u001b[0;32m----> 3\u001b[0m exercise_embedding \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([att(exercise_embed, kc_embed, adj_exercise_kc) \u001b[39mfor\u001b[39;00m att \u001b[39min\u001b[39;00m exercise_kc_attentions], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/knowledge_tracing/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    726\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 727\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    728\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    729\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    730\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    731\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "Cell \u001b[0;32mIn[164], line 39\u001b[0m, in \u001b[0;36mExercise_KC_GraphAttentionLayer.forward\u001b[0;34m(self, exercise_h, kc_h, adj_exercise_kc)\u001b[0m\n\u001b[1;32m     37\u001b[0m e \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleakyrelu(torch\u001b[39m.\u001b[39mmatmul(a_input, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma)\u001b[39m.\u001b[39msqueeze(\u001b[39m2\u001b[39m))\n\u001b[1;32m     38\u001b[0m zero_vec \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m9e15\u001b[39m \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39mones_like(e)\n\u001b[0;32m---> 39\u001b[0m attention \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mwhere(adj_exercise_kc \u001b[39m>\u001b[39;49m \u001b[39m0\u001b[39;49m, e, zero_vec)\n\u001b[1;32m     40\u001b[0m attention \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39msoftmax(attention, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     41\u001b[0m new_kc_embed \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(attention, kc_Wh)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected condition, x and y to be on the same device, but condition is on cuda:0 and x and y are on cpu and cpu respectively"
     ]
    }
   ],
   "source": [
    "exercise_kc_attentions = [\n",
    "    Exercise_KC_GraphAttentionLayer(exercise_embed_dim, exercise_embed_dim, dropout=dropout, alpha=alpha, mode=mode, concat=True) for _ in range(nheads)]\n",
    "exercise_embedding = torch.cat([att(exercise_embed, kc_embed, adj_exercise_kc) for att in exercise_kc_attentions], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor of size (3, 3, 4)\n",
    "x = torch.rand(3, 3, 4)\n",
    "\n",
    "# Reshape the tensor to size (2, 6)\n",
    "\n",
    "\n",
    "# Compute the mean of all the elements in the tensor\n",
    "mean_x = x.mean(2)\n",
    "\n",
    "\n",
    "print(mean_x.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exercise_embedding_add_zero = torch.cat(\n",
    "            [utils.varible(torch.zeros(1, mean_x.shape[1]), gpu), mean_x.cuda()], dim=0)\n",
    "exercise_embedding_add_zero.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1481, 0.2053, 0.4924, 0.1888, 0.2748, 0.9214],\n",
      "        [0.0581, 0.0780, 0.5755, 0.3228, 0.9116, 0.3702],\n",
      "        [0.8120, 0.2454, 0.1920, 0.9954, 0.2558, 0.3684],\n",
      "        [0.6936, 0.8881, 0.7237, 0.2804, 0.1664, 0.3215]])\n",
      "(tensor([[0.1481, 0.2053, 0.4924, 0.1888, 0.2748, 0.9214],\n",
      "        [0.0581, 0.0780, 0.5755, 0.3228, 0.9116, 0.3702]]), tensor([[0.8120, 0.2454, 0.1920, 0.9954, 0.2558, 0.3684],\n",
      "        [0.6936, 0.8881, 0.7237, 0.2804, 0.1664, 0.3215]]))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor of size (3, 3, 4)\n",
    "x = torch.rand(4, 6)\n",
    "chunk_x = torch.chunk(x, 2, 0)\n",
    "print(x)\n",
    "print(chunk_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1366,  1424,  1413,  ...,     0,     0,     0],\n",
      "        [ 6989,  7098,  7069,  ...,  5580,  5759,  5553],\n",
      "        [ 5695,  5806,  5622,  ...,  8711, 12377,  5533],\n",
      "        ...,\n",
      "        [ 5980,  6383,  6464,  ..., 13477, 13488, 13487],\n",
      "        [13479, 13476, 13505,  ..., 14834, 14835, 14836],\n",
      "        [14230, 14369, 14370,  ..., 15978, 15988, 15989]], device='cuda:0')\n",
      "tensor([[ 1366],\n",
      "        [ 6989],\n",
      "        [ 5695],\n",
      "        [ 8682],\n",
      "        [11405],\n",
      "        [13333],\n",
      "        [ 6284],\n",
      "        [11423],\n",
      "        [ 8942],\n",
      "        [ 7063],\n",
      "        [12489],\n",
      "        [12486],\n",
      "        [ 7422],\n",
      "        [ 8075],\n",
      "        [ 5953],\n",
      "        [ 7057],\n",
      "        [ 5940],\n",
      "        [14705],\n",
      "        [ 1994],\n",
      "        [  376],\n",
      "        [ 4527],\n",
      "        [15304],\n",
      "        [12898],\n",
      "        [12466],\n",
      "        [13365],\n",
      "        [ 3637],\n",
      "        [ 5055],\n",
      "        [ 2183],\n",
      "        [ 3045],\n",
      "        [ 5980],\n",
      "        [13479],\n",
      "        [14230]], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqlen = input_exercise.shape[1]\n",
    "slice_exercise_data = torch.chunk(input_exercise, seqlen, 1)\n",
    "print(input_exercise)\n",
    "print(slice_exercise_data[0].squeeze(1).unsqueeze(1))\n",
    "cat_slice_exercise_data = torch.cat([slice_exercise_data[0].squeeze(1).unsqueeze(1),slice_exercise_data[1].squeeze(1).unsqueeze(1)], 1)\n",
    "cat_slice_exercise_data.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slice_exercise_data = torch.chunk(input_exercise, seqlen, 1)\n",
    "len(slice_exercise_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 99, 1])\n",
      "torch.Size([32, 99])\n",
      "tensor([[[0.7773],\n",
      "         [0.6655],\n",
      "         [0.5498],\n",
      "         ...,\n",
      "         [0.8459],\n",
      "         [0.3336],\n",
      "         [0.6329]],\n",
      "\n",
      "        [[0.2650],\n",
      "         [0.6678],\n",
      "         [0.6939],\n",
      "         ...,\n",
      "         [0.6394],\n",
      "         [0.6408],\n",
      "         [0.4079]],\n",
      "\n",
      "        [[0.9580],\n",
      "         [0.2401],\n",
      "         [0.3020],\n",
      "         ...,\n",
      "         [0.8199],\n",
      "         [0.8232],\n",
      "         [0.0746]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.0884],\n",
      "         [0.3532],\n",
      "         [0.4980],\n",
      "         ...,\n",
      "         [0.7674],\n",
      "         [0.3552],\n",
      "         [0.9580]],\n",
      "\n",
      "        [[0.9456],\n",
      "         [0.3261],\n",
      "         [0.1519],\n",
      "         ...,\n",
      "         [0.3355],\n",
      "         [0.1505],\n",
      "         [0.1128]],\n",
      "\n",
      "        [[0.7458],\n",
      "         [0.1261],\n",
      "         [0.2113],\n",
      "         ...,\n",
      "         [0.2993],\n",
      "         [0.9027],\n",
      "         [0.7175]]])\n",
      "tensor([[0.7773, 0.6655, 0.5498,  ..., 0.8459, 0.3336, 0.6329],\n",
      "        [0.2650, 0.6678, 0.6939,  ..., 0.6394, 0.6408, 0.4079],\n",
      "        [0.9580, 0.2401, 0.3020,  ..., 0.8199, 0.8232, 0.0746],\n",
      "        ...,\n",
      "        [0.0884, 0.3532, 0.4980,  ..., 0.7674, 0.3552, 0.9580],\n",
      "        [0.9456, 0.3261, 0.1519,  ..., 0.3355, 0.1505, 0.1128],\n",
      "        [0.7458, 0.1261, 0.2113,  ..., 0.2993, 0.9027, 0.7175]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(32, 99, 1)\n",
    "print(x.shape)\n",
    "x_ = x.squeeze(-1)\n",
    "print(x_.shape)\n",
    "print(x)\n",
    "print(x_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 99])\n",
      "torch.Size([32, 99])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1768])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_1d = target\n",
    "mask = target_1d.ge(0)\n",
    "print(x_.shape)\n",
    "print(mask.shape)\n",
    "filtered_pred = torch.masked_select(x_, mask)\n",
    "filtered_pred.shape\n",
    "filtered_target = torch.masked_select(target_1d, mask)\n",
    "filtered_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6900, device='cuda:0')"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_loss = torch.nn.functional.binary_cross_entropy_with_logits(filtered_pred.cuda(), filtered_target.cuda())\n",
    "predict_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([110, 64])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exercise_embed.shape\n",
    "kc_embed.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "knowledge_tracing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
